# Unified Training Configuration for Sequential Decision-Making Modules

# Data
buffer_path: "data/buffer.pkl"  # Path to replay buffer file (use dummy data if not found)

# State Encoder (Shared across all modules)
encoder_type: "vit"  # Options: "mlp", "cnn", "vit"
latent_dim: 128
encoder_params:
  depth: 4
  heads: 12
  mlp_dim: 512
  pool: "cls"
  dropout: 0.3
  emb_dropout: 0.3
  input_channels: 1
  image_size: [10, 10]

# Color Selection Functions
num_color_selection_fns: 12  # Set this to the number of color selection functions in DSL
num_arc_colors: 10
action_embedding_dim: 32

# Selection Functions
num_selection_fns: 8  # Set this to the number of selection functions in DSL

# Transform Functions
num_transform_actions: 8  # Set this to the number of transform actions in DSL

# Color Predictor Module
color_predictor:
  hidden_dim: 128

# Selection Mask Prediction Module
selection_mask:
  mask_encoder_type: "mlp"  # Options: "mlp", "cnn", "vit"
  latent_mask_dim: 64
  mask_encoder_params:
    mlp:
      num_hidden_layers: 2
      hidden_dim: 128
      activation_fn_str: "relu"
      dropout_rate: 0.1
      input_channels: 1
      image_size: [10, 10]
    cnn:
      num_conv_layers: 2
      base_filters: 16
      kernel_size: 3
      stride: 2
      padding: 1
      activation_fn_str: "relu"
      fc_hidden_dim: null
      dropout_rate: 0.1
      input_channels: 1
      image_size: [10, 10]
    vit:
      depth: 2
      heads: 2
      mlp_dim: 64
      pool: "cls"
      dropout: 0.1
      emb_dropout: 0.1
      patch_size: 2
      input_channels: 1
      image_size: [10, 10]
  selection_mask_predictor_hidden_dim: 128
  use_transformer: false
  transformer_depth: 2
  transformer_heads: 2
  transformer_dim_head: 32
  transformer_mlp_dim: 64
  transformer_dropout: 0.1
  loss_type: "mse"  # Options: "mse", "vicreg"
  use_vicreg: false
  vicreg_sim_coeff: 25.0
  vicreg_std_coeff: 25.0
  vicreg_cov_coeff: 1.0

# Next State Predictor Module
next_state:
  latent_mask_dim: 0  # Set to 0 if not using mask, else specify mask latent dim
  transformer_depth: 2
  transformer_heads: 2
  transformer_dim_head: 64
  transformer_mlp_dim: 128
  transformer_dropout: 0.1
  loss_type: "mse"  # Options: "mse", "vicreg"
  use_vicreg: false
  vicreg_sim_coeff: 25.0
  vicreg_std_coeff: 25.0
  vicreg_cov_coeff: 1.0

# Reward Predictor Module
reward_predictor:
  hidden_dim: 128
  use_transformer: false
  transformer_depth: 2
  transformer_heads: 2
  transformer_dim_head: 64
  transformer_mlp_dim: 128
  transformer_dropout: 0.1
  mlp_hidden_dims: [128, 64]
  activation_fn_str: "relu"
  dropout_rate: 0.1

# Continuation Predictor Module
continuation_predictor:
  hidden_dim: 128
  use_transformer: false
  transformer_depth: 2
  transformer_heads: 2
  transformer_dim_head: 64
  transformer_mlp_dim: 128
  transformer_dropout: 0.1
  mlp_hidden_dims: [128, 64]
  activation_fn_str: "relu"
  dropout_rate: 0.1

# Training (Shared across all modules)
batch_size: 32
num_epochs: 10
learning_rate: 0.001
num_workers: 1
log_interval: 10

# Early Stopping and Model Saving
color_predictor_training:
  early_stopping_patience: 10
  best_model_save_path: "best_model_color_predictor.pth"
selection_predictor_training:
  early_stopping_patience: 10
  best_model_save_path: "best_model_selection_predictor.pth"
next_state_predictor_training:
  early_stopping_patience: 10
  best_model_save_path: "best_model_next_state_predictor.pth"
full_model_training:
  early_stopping_patience: 10
  best_model_save_path: "best_model_full_model.pth" 